{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def read_data(filename):  \n",
    "    df = pd.read_csv(filename)\n",
    "    df.columns = [\"X1\",\"X2\",\"y\"]\n",
    "    X1=df.iloc[:,0]\n",
    "    X2=df.iloc[:,1]\n",
    "    y=df.iloc[:,2]\n",
    "    X=np.column_stack((X1,X2))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, y, title):\n",
    "    fig = plt.figure()\n",
    "    plt.rc('font', size=10)\n",
    "    X_m1 = X[np.where(y == -1)]\n",
    "    X_p1 = X[np.where(y == 1)]\n",
    "    plt.scatter(X_m1[:, 0], X_m1[:, 1], c='g', marker='+',label=\"negative\", s=30)\n",
    "    plt.scatter(X_p1[:, 0], X_p1[:, 1], c='b', marker='+',label=\"positive\", s=30)\n",
    "    plt.xlabel('Feature X1')\n",
    "    plt.ylabel('Feature X2')\n",
    "    plt.legend(loc = 1, framealpha = 1.0)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "X, y = read_data(\"week4.csv\")\n",
    "plot_data(X,y,\"Dataset 1\")\n",
    "\n",
    "X, y = read_data(\"week4_1.csv\")\n",
    "plot_data(X,y,\"Dataset 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i) a)\n",
    "def best_poly_and_c(X, y, title):\n",
    "    fig = plt.figure()\n",
    "    poly_test = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "    Cs = [0.001,0.01,0.05,0.1,0.25,0.5,1,1.5]\n",
    "\n",
    "    for p in poly_test:\n",
    "        avg_accuracy=[]\n",
    "        std_err=[]\n",
    "        poly_x = PolynomialFeatures(p).fit_transform(X)\n",
    "\n",
    "        for c in Cs:\n",
    "            log_reg = LogisticRegression(C=c, max_iter=1000000)\n",
    "            scores = cross_val_score(log_reg, poly_x, y,cv=5, scoring=\"accuracy\")\n",
    "            avg_accuracy.append(scores.mean())\n",
    "            std_err.append(scores.std())\n",
    "\n",
    "        plt.errorbar(Cs, avg_accuracy,  label=\"Degree = {0}\".format(p), yerr=std_err,)\n",
    "        plt.xlabel('C')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc = 1)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "X, y = read_data(\"week4.csv\")\n",
    "best_poly_and_c(X,y, \"Dataset 1\")\n",
    "\n",
    "X, y = read_data(\"week4_1.csv\")\n",
    "best_poly_and_c(X,y,\"Dataset 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i) b)\n",
    "def knn(X,y,title):\n",
    "    fig = plt.figure()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
    "    avg_accuracy=[]\n",
    "    std_err=[]\n",
    "    k_test = np.array(range(1,100))\n",
    "    for k in k_test:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k).fit(x_train,y_train)\n",
    "        scores = cross_val_score(knn, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "        avg_accuracy.append(scores.mean())\n",
    "        std_err.append(scores.std())\n",
    "\n",
    "    plt.errorbar(k_test, avg_accuracy, yerr=std_err)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Predicting k for KNN: ' + title)\n",
    "    plt.show()\n",
    "    \n",
    "X, y = read_data(\"week4.csv\")\n",
    "knn(X,y, \"Dataset 1\")\n",
    "X, y = read_data(\"week4_1.csv\")\n",
    "knn(X,y, \"Dataset 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) c) & d)\n",
    "\n",
    "def c_and_d(X, y, c, k, poly):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # c)\n",
    "    \n",
    "    y_pred_log_reg =[]; y_pred_knn = []\n",
    "    x_poly = PolynomialFeatures(poly).fit_transform(X)\n",
    "    x_poly_train, x_poly_test, y_poly_train, y_poly_test = train_test_split(x_poly, y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    log_reg = LogisticRegression(C=c)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "    log_reg.fit(x_poly_train, y_poly_train)\n",
    "    knn.fit(x_train, y_train)\n",
    "    dummy.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_log_reg = log_reg.predict(x_poly_test)\n",
    "    y_pred_knn = knn.predict(x_test)\n",
    "    y_pred_dummy = dummy.predict(x_test)\n",
    "\n",
    "    log_mat = confusion_matrix(y_poly_test, y_pred_log_reg)\n",
    "    knn_mat = confusion_matrix(y_test, y_pred_knn)\n",
    "    dummy_mat = confusion_matrix(y_test, y_pred_dummy)\n",
    "    \n",
    "    print(\"LogReg \\n\", log_mat)\n",
    "    print(\"Knn \\n\", knn_mat)\n",
    "    print(\"Dummy \\n\", dummy_mat)\n",
    "\n",
    "    # d)\n",
    "    \n",
    "    scores = log_reg.predict_proba(x_poly_test)\n",
    "    fpr, tpr, _ = roc_curve(y_poly_test, scores[:, 1])\n",
    "    plt.plot(fpr, tpr)\n",
    "\n",
    "    scores = knn.predict_proba(x_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores[:, 1])\n",
    "    plt.plot(fpr, tpr)\n",
    "\n",
    "    scores = dummy.predict_proba(x_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores[:, 1])\n",
    "    plt.plot(fpr, tpr)\n",
    "\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(['Logistic Regression', 'KNN', 'Dummy'])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "X, y = read_data(\"week4.csv\")\n",
    "c_and_d(X,y,1,5,2)\n",
    "X, y = read_data(\"week4_1.csv\")\n",
    "c_and_d(X,y,0.1,35,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32b1dc8ad1fb39ea5be90e273f638abf213ab954063c5fcfa6c0bb53d8927201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
